{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Model Development & Training\n",
    "## Weather Emergency Prediction - Rostov Region\n",
    "\n",
    "This notebook handles:\n",
    "- Model training for emergency prediction\n",
    "- Model evaluation and metrics\n",
    "- Feature importance analysis\n",
    "- Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy scikit-learn matplotlib seaborn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data from Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the merged data from Part A\n",
    "try:\n",
    "    merged_df = pd.read_csv('merged_data_rostov.csv')\n",
    "    merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "    print(f\"‚úÖ Loaded {len(merged_df)} records\")\n",
    "    print(f\"Emergency days: {merged_df['has_emergency'].sum()}\")\n",
    "    print(f\"Normal days: {(merged_df['has_emergency']==0).sum()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå File not found. Please run Part A first or upload the file.\")\n",
    "    # Uncomment to upload\n",
    "    # from google.colab import files\n",
    "    # uploaded = files.upload()\n",
    "    # merged_df = pd.read_csv('merged_data_rostov.csv')\n",
    "    # merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def prepare_features(df):\n",
    "    \"\"\"Prepare feature matrix and target variable.\"\"\"\n",
    "    df = df.dropna(subset=['has_emergency'])\n",
    "    \n",
    "    # Exclude non-feature columns\n",
    "    exclude_cols = ['date', 'latitude', 'longitude', 'has_emergency',\n",
    "                   'emergency_type', 'emergency_severity']\n",
    "    \n",
    "    feature_cols = [col for col in df.columns \n",
    "                   if col not in exclude_cols and df[col].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    X = df[feature_cols].fillna(0)\n",
    "    y = df['has_emergency']\n",
    "    \n",
    "    print(f\"Features: {len(feature_cols)}\")\n",
    "    print(f\"Samples: {len(X)}\")\n",
    "    print(f\"Class distribution:\")\n",
    "    print(y.value_counts())\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "X, y, feature_names = prepare_features(merged_df)\n",
    "print(f\"\\n‚úÖ Data prepared: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"‚úÖ Data split and scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model 1: Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "print(\"‚úÖ Random Forest trained\")\n",
    "\n",
    "# Model 2: Gradient Boosting\n",
    "print(\"\\nTraining Gradient Boosting...\")\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "print(\"‚úÖ Gradient Boosting trained\")\n",
    "\n",
    "# Model 3: Logistic Regression\n",
    "print(\"\\nTraining Logistic Regression...\")\n",
    "lr_model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "print(\"‚úÖ Logistic Regression trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model performance.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{model_name} EVALUATION\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"ROC AUC:   {roc_auc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "rf_metrics = evaluate_model(rf_model, X_test_scaled, y_test, \"Random Forest\")\n",
    "gb_metrics = evaluate_model(gb_model, X_test_scaled, y_test, \"Gradient Boosting\")\n",
    "lr_metrics = evaluate_model(lr_model, X_test_scaled, y_test, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Gradient Boosting', 'Logistic Regression'],\n",
    "    'Accuracy': [rf_metrics['accuracy'], gb_metrics['accuracy'], lr_metrics['accuracy']],\n",
    "    'Precision': [rf_metrics['precision'], gb_metrics['precision'], lr_metrics['precision']],\n",
    "    'Recall': [rf_metrics['recall'], gb_metrics['recall'], lr_metrics['recall']],\n",
    "    'F1 Score': [rf_metrics['f1'], gb_metrics['f1'], lr_metrics['f1']],\n",
    "    'ROC AUC': [rf_metrics['roc_auc'], gb_metrics['roc_auc'], lr_metrics['roc_auc']]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "comparison_df.set_index('Model')[['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']].plot(\n",
    "    kind='bar', ax=ax\n",
    ")\n",
    "plt.title('Model Performance Comparison', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ROC Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name, metrics in [('Random Forest', rf_metrics), \n",
    "                             ('Gradient Boosting', gb_metrics),\n",
    "                             ('Logistic Regression', lr_metrics)]:\n",
    "    fpr, tpr, _ = roc_curve(y_test, metrics['y_pred_proba'])\n",
    "    plt.plot(fpr, tpr, label=f\"{model_name} (AUC = {metrics['roc_auc']:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison', fontsize=16, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features:\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Visualize top features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Feature Importance (Random Forest)', fontsize=16, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select best model based on F1 score\n",
    "best_model_name = comparison_df.loc[comparison_df['F1 Score'].idxmax(), 'Model']\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "\n",
    "if best_model_name == 'Random Forest':\n",
    "    best_model = rf_model\n",
    "elif best_model_name == 'Gradient Boosting':\n",
    "    best_model = gb_model\n",
    "else:\n",
    "    best_model = lr_model\n",
    "\n",
    "# Save model and scaler\n",
    "model_data = {\n",
    "    'model': best_model,\n",
    "    'scaler': scaler,\n",
    "    'feature_names': feature_names,\n",
    "    'model_type': best_model_name\n",
    "}\n",
    "\n",
    "joblib.dump(model_data, 'emergency_prediction_model.pkl')\n",
    "print(\"\\n‚úÖ Model saved as 'emergency_prediction_model.pkl'\")\n",
    "\n",
    "# Download model (uncomment if needed)\n",
    "# from google.colab import files\n",
    "# files.download('emergency_prediction_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def predict_emergency(temperature, precipitation, humidity, wind_speed, pressure):\n",
    "    \"\"\"Make a prediction for given weather conditions.\"\"\"\n",
    "    # Load model\n",
    "    model_data = joblib.load('emergency_prediction_model.pkl')\n",
    "    model = model_data['model']\n",
    "    scaler = model_data['scaler']\n",
    "    feature_names = model_data['feature_names']\n",
    "    \n",
    "    # Create input with basic features\n",
    "    input_dict = {\n",
    "        'temperature': temperature,\n",
    "        'precipitation': precipitation,\n",
    "        'humidity': humidity,\n",
    "        'wind_speed': wind_speed,\n",
    "        'pressure': pressure\n",
    "    }\n",
    "    \n",
    "    # Add missing features as 0\n",
    "    for feat in feature_names:\n",
    "        if feat not in input_dict:\n",
    "            input_dict[feat] = 0.0\n",
    "    \n",
    "    # Create DataFrame\n",
    "    input_df = pd.DataFrame([input_dict])[feature_names]\n",
    "    \n",
    "    # Scale and predict\n",
    "    input_scaled = scaler.transform(input_df)\n",
    "    prediction = model.predict(input_scaled)[0]\n",
    "    probability = model.predict_proba(input_scaled)[0]\n",
    "    \n",
    "    return {\n",
    "        'will_occur': bool(prediction),\n",
    "        'probability': float(probability[1]),\n",
    "        'confidence': float(max(probability))\n",
    "    }\n",
    "\n",
    "# Test prediction\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEST PREDICTIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test case 1: Extreme heat\n",
    "result1 = predict_emergency(temperature=38, precipitation=0, humidity=20, wind_speed=3, pressure=1010)\n",
    "print(f\"\\nCase 1: Extreme heat (38¬∞C, low humidity)\")\n",
    "print(f\"Prediction: {'‚ö†Ô∏è EMERGENCY' if result1['will_occur'] else '‚úÖ NO EMERGENCY'}\")\n",
    "print(f\"Probability: {result1['probability']:.2%}\")\n",
    "print(f\"Confidence: {result1['confidence']:.2%}\")\n",
    "\n",
    "# Test case 2: Normal conditions\n",
    "result2 = predict_emergency(temperature=20, precipitation=5, humidity=60, wind_speed=4, pressure=1013)\n",
    "print(f\"\\nCase 2: Normal conditions (20¬∞C, moderate humidity)\")\n",
    "print(f\"Prediction: {'‚ö†Ô∏è EMERGENCY' if result2['will_occur'] else '‚úÖ NO EMERGENCY'}\")\n",
    "print(f\"Probability: {result2['probability']:.2%}\")\n",
    "print(f\"Confidence: {result2['confidence']:.2%}\")\n",
    "\n",
    "# Test case 3: Heavy rain\n",
    "result3 = predict_emergency(temperature=15, precipitation=80, humidity=90, wind_speed=10, pressure=995)\n",
    "print(f\"\\nCase 3: Heavy rain (80mm precipitation)\")\n",
    "print(f\"Prediction: {'‚ö†Ô∏è EMERGENCY' if result3['will_occur'] else '‚úÖ NO EMERGENCY'}\")\n",
    "print(f\"Probability: {result3['probability']:.2%}\")\n",
    "print(f\"Confidence: {result3['confidence']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ **Completed:**\n",
    "- Trained 3 different models (Random Forest, Gradient Boosting, Logistic Regression)\n",
    "- Evaluated models on multiple metrics\n",
    "- Compared model performance\n",
    "- Analyzed feature importance\n",
    "- Saved best model for deployment\n",
    "- Created prediction function\n",
    "\n",
    "**Next:** Continue to Part C - API Development or Part D - Web Interface"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
