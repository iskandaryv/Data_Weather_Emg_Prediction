{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Load Competition Data (CSV/Excel)\n",
    "## Weather Emergency Prediction - Load Existing Files\n",
    "\n",
    "**Use this notebook when you have competition data files.**\n",
    "\n",
    "This notebook handles:\n",
    "- ‚úÖ Load CSV/Excel files with geodata\n",
    "- ‚úÖ Auto-detect latitude/longitude columns\n",
    "- ‚úÖ Create GeoDataFrames from existing data\n",
    "- ‚úÖ Spatial operations and feature engineering\n",
    "- ‚úÖ Data validation and cleaning\n",
    "- ‚úÖ Export processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy geopandas shapely folium matplotlib seaborn plotly scikit-learn openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"GeoPandas version: {gpd.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Competition Data from CSV/Excel\n",
    "\n",
    "This function automatically detects and loads your data files with geodata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_competition_data(filepath, lat_col=None, lon_col=None, date_col='date'):\n",
    "    \"\"\"\n",
    "    Load competition CSV/Excel file with geodata.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to CSV or Excel file\n",
    "        lat_col: Name of latitude column (auto-detected if None)\n",
    "        lon_col: Name of longitude column (auto-detected if None)\n",
    "        date_col: Name of date column\n",
    "    \n",
    "    Returns:\n",
    "        GeoDataFrame with loaded data\n",
    "    \"\"\"\n",
    "    print(f\"üìÇ Loading data from: {filepath}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Load file based on extension\n",
    "    if filepath.endswith('.csv'):\n",
    "        df = pd.read_csv(filepath)\n",
    "    elif filepath.endswith(('.xlsx', '.xls')):\n",
    "        df = pd.read_excel(filepath)\n",
    "    else:\n",
    "        raise ValueError(\"File must be CSV or Excel format\")\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(df):,} rows, {len(df.columns)} columns\")\n",
    "    print(f\"\\nColumns found: {list(df.columns)}\")\n",
    "    \n",
    "    # Auto-detect lat/lon columns if not specified\n",
    "    if lat_col is None or lon_col is None:\n",
    "        print(f\"\\nüîç Auto-detecting geodata columns...\")\n",
    "        \n",
    "        lat_candidates = [col for col in df.columns if 'lat' in col.lower()]\n",
    "        lon_candidates = [col for col in df.columns if 'lon' in col.lower()]\n",
    "        \n",
    "        if lat_candidates and lon_candidates:\n",
    "            lat_col = lat_candidates[0]\n",
    "            lon_col = lon_candidates[0]\n",
    "            print(f\"   ‚úÖ Found geodata: {lat_col}, {lon_col}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Could not find latitude/longitude columns. Please specify manually.\")\n",
    "    \n",
    "    # Check if columns exist\n",
    "    if lat_col not in df.columns or lon_col not in df.columns:\n",
    "        raise ValueError(f\"Columns not found: {lat_col}, {lon_col}\")\n",
    "    \n",
    "    # Parse date column if exists\n",
    "    if date_col in df.columns:\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "        print(f\"   ‚úÖ Parsed date column: {date_col}\")\n",
    "        print(f\"   üìÖ Date range: {df[date_col].min()} to {df[date_col].max()}\")\n",
    "    \n",
    "    # Remove rows with invalid coordinates\n",
    "    initial_len = len(df)\n",
    "    df = df.dropna(subset=[lat_col, lon_col])\n",
    "    if len(df) < initial_len:\n",
    "        print(f\"   üóëÔ∏è Removed {initial_len - len(df)} rows with missing coordinates\")\n",
    "    \n",
    "    # Create Point geometries\n",
    "    geometry = [Point(lon, lat) for lon, lat in zip(df[lon_col], df[lat_col])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs='EPSG:4326')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Created GeoDataFrame:\")\n",
    "    print(f\"   Total points: {len(gdf):,}\")\n",
    "    print(f\"   Unique locations: {gdf.geometry.nunique()}\")\n",
    "    print(f\"   Coordinate bounds: {gdf.total_bounds}\")\n",
    "    print(f\"   CRS: {gdf.crs}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "print(\"‚úÖ Data loader function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Your Competition Files\n",
    "\n",
    "**Option 1:** Specify file path directly\n",
    "\n",
    "**Option 2:** Upload file (for Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Load from file path\n",
    "# Replace with your actual file path\n",
    "weather_gdf = load_competition_data('sample_weather.xlsx')\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nüìä Data Preview:\")\n",
    "weather_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: Upload file (Google Colab)\n",
    "# Uncomment to use:\n",
    "\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()\n",
    "# filename = list(uploaded.keys())[0]\n",
    "# weather_gdf = load_competition_data(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Validation & Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DATA VALIDATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìä Basic Statistics:\")\n",
    "print(f\"   Total records: {len(weather_gdf):,}\")\n",
    "print(f\"   Total columns: {len(weather_gdf.columns)}\")\n",
    "print(f\"   Memory usage: {weather_gdf.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nüìç Geographic Coverage:\")\n",
    "print(f\"   Unique locations: {weather_gdf.geometry.nunique()}\")\n",
    "print(f\"   Bounds (lon_min, lat_min, lon_max, lat_max): {weather_gdf.total_bounds}\")\n",
    "\n",
    "print(f\"\\nüîç Data Quality:\")\n",
    "print(f\"   Missing values per column:\")\n",
    "missing = weather_gdf.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"   ‚úÖ No missing values\")\n",
    "\n",
    "print(f\"\\nüìà Numeric Columns Summary:\")\n",
    "numeric_cols = weather_gdf.select_dtypes(include=[np.number]).columns\n",
    "print(weather_gdf[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spatial_features(gdf):\n",
    "    \"\"\"Add spatial features to GeoDataFrame.\"\"\"\n",
    "    print(\"üó∫Ô∏è Adding spatial features...\")\n",
    "    \n",
    "    gdf = gdf.copy()\n",
    "    \n",
    "    # Calculate center point\n",
    "    center_lat = gdf.geometry.y.mean()\n",
    "    center_lon = gdf.geometry.x.mean()\n",
    "    print(f\"   Center: ({center_lat:.4f}, {center_lon:.4f})\")\n",
    "    \n",
    "    # Distance to center\n",
    "    center_point = Point(center_lon, center_lat)\n",
    "    gdf_proj = gdf.to_crs('EPSG:3857')\n",
    "    center_proj = gpd.GeoSeries([center_point], crs='EPSG:4326').to_crs('EPSG:3857')[0]\n",
    "    gdf['distance_to_center_km'] = gdf_proj.geometry.distance(center_proj) / 1000\n",
    "    \n",
    "    # Extract coordinates\n",
    "    gdf['lat'] = gdf.geometry.y\n",
    "    gdf['lon'] = gdf.geometry.x\n",
    "    \n",
    "    # Normalize coordinates\n",
    "    gdf['lat_norm'] = (gdf['lat'] - gdf['lat'].mean()) / gdf['lat'].std()\n",
    "    gdf['lon_norm'] = (gdf['lon'] - gdf['lon'].mean()) / gdf['lon'].std()\n",
    "    \n",
    "    print(f\"   ‚úÖ Added: distance_to_center_km, lat, lon, lat_norm, lon_norm\")\n",
    "    print(f\"   Distance range: {gdf['distance_to_center_km'].min():.2f} - {gdf['distance_to_center_km'].max():.2f} km\")\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "weather_gdf = add_spatial_features(weather_gdf)\n",
    "weather_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_temporal_features(gdf, date_col='date'):\n",
    "    \"\"\"Add temporal features to GeoDataFrame.\"\"\"\n",
    "    print(\"üìÖ Adding temporal features...\")\n",
    "    \n",
    "    if date_col not in gdf.columns:\n",
    "        print(f\"   ‚ö†Ô∏è Date column '{date_col}' not found. Skipping.\")\n",
    "        return gdf\n",
    "    \n",
    "    gdf = gdf.copy()\n",
    "    \n",
    "    gdf['year'] = gdf[date_col].dt.year\n",
    "    gdf['month'] = gdf[date_col].dt.month\n",
    "    gdf['day'] = gdf[date_col].dt.day\n",
    "    gdf['day_of_year'] = gdf[date_col].dt.dayofyear\n",
    "    gdf['week_of_year'] = gdf[date_col].dt.isocalendar().week.astype(int)\n",
    "    gdf['season'] = (gdf['month'] % 12 // 3 + 1)\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    gdf['month_sin'] = np.sin(2 * np.pi * gdf['month'] / 12)\n",
    "    gdf['month_cos'] = np.cos(2 * np.pi * gdf['month'] / 12)\n",
    "    gdf['day_sin'] = np.sin(2 * np.pi * gdf['day_of_year'] / 365)\n",
    "    gdf['day_cos'] = np.cos(2 * np.pi * gdf['day_of_year'] / 365)\n",
    "    \n",
    "    print(f\"   ‚úÖ Added: year, month, day, season, cyclical encodings\")\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "weather_gdf = add_temporal_features(weather_gdf)\n",
    "weather_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Data on Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_map(gdf, max_points=100):\n",
    "    \"\"\"Create interactive map of data points.\"\"\"\n",
    "    print(f\"üó∫Ô∏è Creating interactive map...\")\n",
    "    \n",
    "    # Sample data if too many points\n",
    "    if len(gdf) > max_points:\n",
    "        gdf_sample = gdf.sample(max_points, random_state=42)\n",
    "        print(f\"   Showing {max_points} random points (out of {len(gdf):,})\")\n",
    "    else:\n",
    "        gdf_sample = gdf\n",
    "    \n",
    "    # Calculate center\n",
    "    center_lat = gdf.geometry.y.mean()\n",
    "    center_lon = gdf.geometry.x.mean()\n",
    "    \n",
    "    # Create map\n",
    "    m = folium.Map(\n",
    "        location=[center_lat, center_lon],\n",
    "        zoom_start=10,\n",
    "        tiles='CartoDB positron'\n",
    "    )\n",
    "    \n",
    "    # Add points\n",
    "    for idx, row in gdf_sample.iterrows():\n",
    "        popup_text = f\"<b>Point {idx}</b><br>\"\n",
    "        for col in ['date', 'temperature', 'precipitation', 'district_id', 'district']:\n",
    "            if col in row:\n",
    "                popup_text += f\"{col}: {row[col]}<br>\"\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=[row.geometry.y, row.geometry.x],\n",
    "            radius=5,\n",
    "            popup=popup_text,\n",
    "            color='blue',\n",
    "            fill=True,\n",
    "            fillOpacity=0.6\n",
    "        ).add_to(m)\n",
    "    \n",
    "    print(f\"   ‚úÖ Map created\")\n",
    "    return m\n",
    "\n",
    "map_obj = create_map(weather_gdf, max_points=100)\n",
    "map_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Exporting processed data...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Export as GeoJSON (with geometry)\n",
    "weather_gdf.to_file('processed_data.geojson', driver='GeoJSON')\n",
    "print(\"‚úÖ processed_data.geojson\")\n",
    "\n",
    "# Export as CSV (without geometry)\n",
    "weather_df = weather_gdf.drop('geometry', axis=1)\n",
    "weather_df.to_csv('processed_data.csv', index=False)\n",
    "print(f\"‚úÖ processed_data.csv ({len(weather_df):,} rows)\")\n",
    "\n",
    "# Export feature list\n",
    "with open('feature_list.txt', 'w') as f:\n",
    "    f.write(\"FEATURES LIST\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    for i, col in enumerate(weather_df.columns, 1):\n",
    "        f.write(f\"{i:3d}. {col}\\n\")\n",
    "print(f\"‚úÖ feature_list.txt ({len(weather_df.columns)} features)\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n‚úÖ All files exported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìä Processed Data:\")\n",
    "print(f\"   Total records: {len(weather_gdf):,}\")\n",
    "print(f\"   Total features: {len(weather_gdf.columns)}\")\n",
    "print(f\"   Unique locations: {weather_gdf.geometry.nunique()}\")\n",
    "\n",
    "if 'date' in weather_gdf.columns:\n",
    "    print(f\"\\nüìÖ Temporal Coverage:\")\n",
    "    print(f\"   Date range: {weather_gdf['date'].min()} to {weather_gdf['date'].max()}\")\n",
    "    print(f\"   Total days: {(weather_gdf['date'].max() - weather_gdf['date'].min()).days}\")\n",
    "\n",
    "print(f\"\\nüìç Geographic Coverage:\")\n",
    "print(f\"   Latitude range: {weather_gdf.geometry.y.min():.4f} to {weather_gdf.geometry.y.max():.4f}\")\n",
    "print(f\"   Longitude range: {weather_gdf.geometry.x.min():.4f} to {weather_gdf.geometry.x.max():.4f}\")\n",
    "print(f\"   Distance spread: {weather_gdf['distance_to_center_km'].max():.2f} km\")\n",
    "\n",
    "print(f\"\\nüîß Features Added:\")\n",
    "print(f\"   Spatial: lat, lon, lat_norm, lon_norm, distance_to_center_km\")\n",
    "if 'month_sin' in weather_gdf.columns:\n",
    "    print(f\"   Temporal: year, month, day, season, cyclical encodings\")\n",
    "\n",
    "print(f\"\\nüìÅ Output Files:\")\n",
    "print(f\"   ‚Ä¢ processed_data.geojson (with geometry)\")\n",
    "print(f\"   ‚Ä¢ processed_data.csv (tabular data)\")\n",
    "print(f\"   ‚Ä¢ feature_list.txt (feature names)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ PART A COMPLETED - Ready for Model Training!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
